{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgT/lFxOqAWeINhDDFrFev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vridhi-Wadhawan/bank-marketing-mlops/blob/main/ML_Ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Operations (MLOps)\n",
        "\n",
        "**Author:** Vridhi Wadhawan"
      ],
      "metadata": {
        "id": "d2A5dtHv_h8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose\n",
        "Model training, feature engineering, and evaluation for the bank marketing prediction task."
      ],
      "metadata": {
        "id": "rxSffx0v7hHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Understanding"
      ],
      "metadata": {
        "id": "m_aanPSI-nw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Business Problem :**\n",
        "    Improve efficiency of term deposit marketing campaigns by targeting customers with higher likelihood to subscribe.\n",
        "\n",
        "* **Objective :**\n",
        "    Deploy a model to maximize the F1 Score such that ensuring a strong balance between precision and recall — while minimizing the number of unnecessary customer contacts\n",
        "\n",
        "* **Constraints :**\n",
        "    * Limited marketing budget restricts the number of contacts\n",
        "    * High false positives (contacting uninterested customers) must be minimized to preserve resources and customer goodwill.\n",
        "\n",
        "* **Success Metrics :**\n",
        "    * *Business:* Atleast 10% reduction in ineffective outreach\n",
        "    * *ML:* Improve an F1 Score ≥ 0.40\n",
        "    * *Economic:* Projected annual ROI >15% through smarter targetting"
      ],
      "metadata": {
        "id": "0u3VNCg_PslR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "sFXsE4vybAAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "!pip install feature_engine\n",
        "!pip install dtale\n",
        "!pip install -U dtale\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import feature_engine\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro, chi2_contingency\n",
        "import warnings\n",
        "import dtale\n",
        "import pickle\n",
        "import time\n",
        "import joblib\n",
        "import dtale.app as dtale_app\n",
        "# Set the USE_COLAB flag to True\n",
        "dtale_app.USE_COLAB = True\n",
        "\n",
        "# Modeling imports for later\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"viridis\")"
      ],
      "metadata": {
        "id": "0gxJO3E2bAAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading And Initial Exploration\n"
      ],
      "metadata": {
        "id": "-_oVm4dTbAAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading And Initial Exploration"
      ],
      "metadata": {
        "id": "YskGDXuPH4pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading And Initial Exploration\n",
        "def load_and_explore_data(file_path):\n",
        "    \"\"\"Load dataset and perform initial exploration\"\"\"\n",
        "    print(\"*\"*50)\n",
        "    print(\"LOADING AND EXPLORING DATA\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Basic info\n",
        "    print(\"Basic Info\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(f\"\\nDataset info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Check target distribution\n",
        "    if 'y' in df.columns:\n",
        "        print(f\"\\nTarget variable distribution:\")\n",
        "        print(df['y'].value_counts())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "144TAb7fbAAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN8OAKEybAAg"
      },
      "source": [
        "df = load_and_explore_data('/content/bank-additional.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Business & Statistical Insights"
      ],
      "metadata": {
        "id": "0h_np--Y38_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Overview**\n",
        "- Total records: 4,119 customers\n",
        "- Total features: 21 columns (mix of numerical and categorical)\n",
        "- Each row represents one customer who was contacted for a term deposit campaign\n",
        "\n",
        "**Business Insights**\n",
        "- Target variable ('y') is highly imbalanced:\n",
        "  - 3,668 customers (≈89%) did not subscribe\n",
        "  - 451 customers (≈11%) subscribed\n",
        "- The Imblance implies that most of the outreach efforts are not converting\n",
        "- Predictive modeling can help:\n",
        "  - Identify likely responders\n",
        "  - Reduce marketing costs\n",
        "  - Improve conversion rates\n",
        "\n",
        "**Statistical Insights**\n",
        "- No missing values in the dataset\n",
        "- Feature types:\n",
        "  - Numerical: age, duration, euribor3m, etc.\n",
        "  - Categorical: job, marital status, contact type, etc.\n",
        "- Economic indicators included:\n",
        "  - emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed\n",
        "  - Economic conditions may influence customer decisions"
      ],
      "metadata": {
        "id": "mRYiLi1Q2e6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning And Pre-Processing"
      ],
      "metadata": {
        "id": "EaOk9p0ybAAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Cleaning"
      ],
      "metadata": {
        "id": "erg4F4NI62oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "def clean_data(df):\n",
        "    \"\"\"Performing comprehensive data cleaning\"\"\"\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(\"DATA CLEANING\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    # Check for duplicates\n",
        "    print(f\"Checking for duplicate rows...\")\n",
        "    duplicate_count = df.duplicated().sum()\n",
        "    print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "    if duplicate_count > 0:\n",
        "        df = df.drop_duplicates()\n",
        "        print(f\"Removed {duplicate_count} duplicate rows\")\n",
        "\n",
        "    # Check for missing values\n",
        "    print(f\"\\nChecking for missing values...\")\n",
        "    missing_summary = df.isnull().sum()\n",
        "    print(f\"\\nMissing values per column:\\n{missing_summary[missing_summary > 0]}\")\n",
        "\n",
        "    # Check for 'unknown' values\n",
        "    print(f\"\\nChecking for 'unknown' values...\")\n",
        "    unknown_counts = {}\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            unknown_count = (df[col] == 'unknown').sum()\n",
        "            if unknown_count > 0:\n",
        "                unknown_counts[col] = unknown_count\n",
        "\n",
        "    if unknown_counts:\n",
        "        print(f\"\\n'Unknown' values found:\")\n",
        "        for col, count in unknown_counts.items():\n",
        "            pct = count / len(df) * 100\n",
        "            print(f\"{col}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "vRfgPj2abAAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = clean_data(df)"
      ],
      "metadata": {
        "id": "csrsTXGBbAAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "XbZC22LH5v1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Overview**\n",
        "\n",
        "Duplicate And Missing Values\n",
        "- No duplicate rows found — data quality is intact.\n",
        "- No missing values in any column — no need for imputation.\n",
        "\n",
        "'Unknown' Values (Treated as Missing)\n",
        "- Some columns contain 'unknown' as a placeholder:\n",
        "  - default: 803 entries (19.5%)\n",
        "  - education: 167 entries (4.05%)\n",
        "  - housing & loan: 105 each (2.55%)\n",
        "  - job: 39 entries (0.95%)\n",
        "  - marital: 11 entries (0.27%)\n",
        "\n",
        "**Business Insights**\n",
        "- High number of 'unknown' values in default column might indicate that the customers are reluctant to disclose financial risk — which could be a red flag for targeting\n",
        "- Education and job-related unknowns suggest incomplete profiling, which may inturn affect how well the bank understands its customer base\n",
        "\n",
        "**Statistical Insights**\n",
        "- Even though there are no nulls, 'unknown' values need to be handled carefully (e.g., treated as a separate category or imputed).\n",
        "- Clean data ensures more reliable model training and evaluation."
      ],
      "metadata": {
        "id": "C45p2VFY5rfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feaature Engneering And Pre-Processing"
      ],
      "metadata": {
        "id": "4UH9JTSx6tL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feaature Engneering And Pre-Processing\n",
        "def feature_engineering(df):\n",
        "    \"\"\"Create new features and transform existing ones\"\"\"\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(\"FEATURE ENGINEERING\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    # Create binary target\n",
        "    df['y_binary'] = df['y'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "    # Behavioral features\n",
        "    df['recently_contacted'] = df['pdays'].apply(lambda x: 0 if x == 999 else 1)\n",
        "    df['contact_intensity'] = df['campaign'] / (df['duration'] + 1)\n",
        "\n",
        "    # Employment and risk features\n",
        "    employed_jobs = ['admin.', 'technician', 'management', 'services', 'blue-collar']\n",
        "    df['is_employed'] = df['job'].isin(employed_jobs).astype(int)\n",
        "    df['is_risk_group'] = ((df['default'] == 'yes') | (df['loan'] == 'yes')).astype(int)\n",
        "\n",
        "    # Temporal features\n",
        "    week_mapping = {\n",
        "        'mon': 'early', 'tue': 'early', 'wed': 'mid',\n",
        "        'thu': 'late', 'fri': 'late'\n",
        "    }\n",
        "    df['week_segment'] = df['day_of_week'].map(week_mapping)\n",
        "\n",
        "    def map_season(month):\n",
        "        if month in ['dec', 'jan', 'feb']:\n",
        "            return 'winter'\n",
        "        elif month in ['mar', 'apr', 'may']:\n",
        "            return 'spring'\n",
        "        elif month in ['jun', 'jul', 'aug']:\n",
        "            return 'summer'\n",
        "        else:\n",
        "            return 'autumn'\n",
        "\n",
        "    df['season_category'] = df['month'].apply(map_season)\n",
        "\n",
        "    # Economic pressure index\n",
        "    df['economic_pressure_index'] = (\n",
        "        (df['cons.price.idx'] - df['cons.conf.idx']) /\n",
        "        (df['emp.var.rate'] + 1 + 1e-6)\n",
        "    )\n",
        "\n",
        "    # Age groups\n",
        "    df['age_group'] = pd.cut(df['age'],\n",
        "                            bins=[0, 30, 45, 60, 100],\n",
        "                            labels=['young', 'middle', 'senior', 'elderly'])\n",
        "\n",
        "    print(f\"Created {len([col for col in df.columns if col not in ['y']])} features\")\n",
        "\n",
        "    # Segregate variables\n",
        "    date_vars = ['month', 'day_of_week','week_segment', 'season_category']\n",
        "    numeric_vars = ['age', 'duration', 'campaign', 'pdays', 'previous','emp.var.rate', 'cons.price.idx', 'cons.conf.idx','euribor3m', 'nr.employed','contact_intensity', 'economic_pressure_index']\n",
        "    categorical_vars = ['job', 'marital', 'education', 'default','housing', 'loan', 'contact', 'poutcome','recently_contacted', 'is_employed', 'is_risk_group','y','y_binary','age_group']\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Date Variables:\", date_vars)\n",
        "    print(\"Numeric Variables:\", numeric_vars)\n",
        "    print(\"Categorical Variables:\", categorical_vars)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    df.info()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "s1xpMI4VbAAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = feature_engineering(df)"
      ],
      "metadata": {
        "id": "14Yo2jJQbAAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "CRoL-kas7AJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Overview**\n",
        "\n",
        "Feature Summary\n",
        "- Total features after engineering: 30\n",
        "- New features created: 9\n",
        "  - y_binary, recently_contacted, contact_intensity, is_employed, is_risk_group, week_segment, season_category, economic_pressure_index, age_group\n",
        "\n",
        "Classification of Feature Types\n",
        "- Date-based: month, day_of_week, week_segment, season_category\n",
        "- Numeric: age, duration, pdays, euribor3m, etc.\n",
        "- Categorical: job, marital, education, contact, etc.\n",
        "- Binary flags: is_employed, recently_contacted, y_binary\n",
        "\n",
        "**Business Insights**\n",
        "\n",
        "- New features like is_risk_group and economic_pressure_index help the bank assess customer risk and economic sensitivity which is useful for targeting\n",
        "- Segmenting by week or season can help reveal when customers are more responsive to campaigns\n",
        "- Flags like recently_contacted prevent over-contacting the same customer, improving customer experience.\n",
        "\n",
        "**Statistical Insights**\n",
        "- Feature diversity (categorical, numeric, temporal) helps to enriche the model’s ability to learn patterns.\n",
        "- Binary encoding (e.g., y_binary) simplifies classification tasks\n",
        "- Engineered features add business context and helps to improve model interpretability\n"
      ],
      "metadata": {
        "id": "UUW_p_iX5i7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "qZEKODrTbAAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "MRLpYPZNCF0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segregate variables\n",
        "date_vars = ['month', 'day_of_week','week_segment', 'season_category']\n",
        "numeric_vars = ['age', 'duration', 'campaign', 'pdays', 'previous','emp.var.rate', 'cons.price.idx', 'cons.conf.idx','euribor3m', 'nr.employed','contact_intensity', 'economic_pressure_index']\n",
        "categorical_vars = ['job', 'marital', 'education', 'default','housing', 'loan', 'contact', 'poutcome','recently_contacted', 'is_employed', 'is_risk_group','y','y_binary','age_group']"
      ],
      "metadata": {
        "id": "bXIXtwnWbAAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive exploratory data analysis with visualizations\n",
        "Parameters:\n",
        "* df (DataFrame): Input dataset\n",
        "* numeric_vars (list): List of numerical variable names\n",
        "* categorical_vars (list): List of categorical variable names\n",
        "* date_vars (list): List of date-related variable names"
      ],
      "metadata": {
        "id": "FifIecTHbAAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_eda(df):\n",
        "\n",
        "    # Validate input\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame\")\n",
        "\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    sns.light_palette(\"navy\")\n",
        "\n",
        "    # Target analysis\n",
        "    print(f\"\\nTARGET VARIABLE ANALYSIS:\")\n",
        "    target_dist = df['y_binary'].value_counts()\n",
        "    print(f\"Distribution:\\n{target_dist}\")\n",
        "    print(f\"Positive rate: {df['y_binary'].mean():.3f}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Numerical Variables Analysis\n",
        "    print(\"\\nNUMERICAL VARIABLES SUMMARY\")\n",
        "    numeric_summary = df[numeric_vars].describe().T\n",
        "    numeric_summary['median'] = df[numeric_vars].median()\n",
        "    numeric_summary['mode'] = df[numeric_vars].mode().iloc[0]\n",
        "    numeric_summary['variance'] = df[numeric_vars].var()\n",
        "    numeric_summary['range'] = numeric_summary['max'] - numeric_summary['min']\n",
        "    numeric_summary['iqr'] = numeric_summary['75%'] - numeric_summary['25%']\n",
        "    numeric_summary['outliers'] = numeric_summary['iqr'] * 1.5\n",
        "    numeric_summary['skewness'] = df[numeric_vars].skew()\n",
        "    numeric_summary['kurtosis'] = df[numeric_vars].kurtosis()\n",
        "    display(numeric_summary)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    for col in numeric_vars:\n",
        "        plt.figure(figsize=(15, 4))\n",
        "        # Histogram + KDE\n",
        "        plt.subplot(1, 3, 1)\n",
        "        sns.histplot(df[col], kde=True, bins=30)\n",
        "        plt.axvline(df[col].mean(), color='red', ls='--')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        # Boxplot\n",
        "        plt.subplot(1, 3, 2)\n",
        "        sns.boxplot(x=df[col])\n",
        "        plt.axvline(df[col].mean(), color='red', ls='--')\n",
        "        plt.title(f'Boxplot of {col}')\n",
        "        # Q-Q Plot\n",
        "        plt.subplot(1, 3, 3)\n",
        "        stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)\n",
        "        plt.title(f'Q-Q Plot of {col}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Categorical Variables Analysis\n",
        "    print(\"\\nCATEGORICAL VARIABLES SUMMARY\")\n",
        "    for col in categorical_vars:\n",
        "        print(f\"\\n{col}:\\n\", df[col].value_counts(dropna=False))\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    plt.figure(figsize=(15, 5*math.ceil(len(categorical_vars)/3)))\n",
        "    for i, col in enumerate(categorical_vars, 1):\n",
        "        ax = plt.subplot(math.ceil(len(categorical_vars)/3), 3, i)\n",
        "        order = df[col].value_counts().index\n",
        "        sns.countplot(data=df, x=col, order=order, ax=ax)\n",
        "        ax.set_title(col)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        for bar in ax.patches:\n",
        "            plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(),\n",
        "                     int(bar.get_height()), ha='center', va='bottom', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Date Variables Analysis\n",
        "    print(\"\\nDATE VARIABLES ANALYSIS\")\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    # Month\n",
        "    plt.subplot(2, 2, 1)\n",
        "    ax = sns.countplot(data=df, x='month',\n",
        "                order=['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'])\n",
        "    for bar in ax.patches:\n",
        "        plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(),\n",
        "                 int(bar.get_height()), ha='center', va='bottom')\n",
        "    plt.title(\"Contacts by Month\")\n",
        "    # Day of Week\n",
        "    plt.subplot(2, 2, 2)\n",
        "    ax = sns.countplot(data=df, x='day_of_week',\n",
        "                order=['mon','tue','wed','thu','fri'])\n",
        "    for bar in ax.patches:\n",
        "        plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(),\n",
        "                 int(bar.get_height()), ha='center', va='bottom')\n",
        "    plt.title(\"Contacts by Day\")\n",
        "    # Week Segment\n",
        "    plt.subplot(2, 2, 3)\n",
        "    ax = sns.countplot(data=df, x='week_segment',\n",
        "                order=['early','mid','late'])\n",
        "    for bar in ax.patches:\n",
        "        plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(),\n",
        "                 int(bar.get_height()), ha='center', va='bottom')\n",
        "    plt.title(\"Week Segment\")\n",
        "    # Season\n",
        "    plt.subplot(2, 2, 4)\n",
        "    ax = sns.countplot(data=df, x='season_category',\n",
        "                order=['winter','spring','summer','autumn'])\n",
        "    for bar in ax.patches:\n",
        "        plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(),\n",
        "                 int(bar.get_height()), ha='center', va='bottom')\n",
        "    plt.title(\"Season\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bivariate Analysis: Numeric vs Target\n",
        "    print(\"\\nBIVARIATE ANALYSIS: NUMERIC VS TARGET\")\n",
        "    plt.figure(figsize=(20, 4*math.ceil(len(numeric_vars)/4)))\n",
        "    for i, col in enumerate(numeric_vars, 1):\n",
        "        ax = plt.subplot(math.ceil(len(numeric_vars)/4), 4, i)\n",
        "        sns.boxplot(x='y', y=col, data=df, showmeans=True,\n",
        "                    meanprops={\"markerfacecolor\":\"red\"})\n",
        "        ax.set_title(f'{col} by Subscription')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bivariate Analysis: Categorical vs Target\n",
        "    print(\"\\nBIVARIATE ANALYSIS: CATEGORICAL VS TARGET\")\n",
        "    avg = df['y_binary'].mean()\n",
        "    plt.figure(figsize=(18, 6*math.ceil(len(categorical_vars)/3)))\n",
        "    for i, col in enumerate(categorical_vars, 1):\n",
        "        ax = plt.subplot(math.ceil(len(categorical_vars)/3), 3, i)\n",
        "        order = df.groupby(col)['y_binary'].mean().sort_values().index\n",
        "        sns.barplot(x=col, y='y_binary', data=df, order=order, ax=ax)\n",
        "        ax.axhline(avg, color='red', ls='--')\n",
        "        ax.set_title(f'Subscription Rate by {col}')\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Multivariate Analysis\n",
        "    print(\"\\nMULTIVARIATE ANALYSIS\")\n",
        "\n",
        "    # Correlation Matrix\n",
        "    corr = df[numeric_vars + ['y_binary']].corr()\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "    plt.title(\"Correlation Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance by correlation\n",
        "    target_corr = abs(corr['y_binary'].drop('y_binary')).sort_values(ascending=False)\n",
        "    print(f\"\\nTop features by correlation with target:\")\n",
        "    print(target_corr.head(10))\n",
        "\n",
        "    # Chi-square Tests\n",
        "    print(\"\\nCHI-SQUARE TEST RESULTS (p-values):\")\n",
        "    for col in categorical_vars:\n",
        "        ct = pd.crosstab(df[col], df['y_binary'])\n",
        "        _, p, _, _ = chi2_contingency(ct)\n",
        "        print(f\"{col}: p = {p:.4f}\")\n",
        "\n",
        "    # Dataset Info\n",
        "    print(\"\\nDATASET INFORMATION:\")\n",
        "    return df.info()"
      ],
      "metadata": {
        "id": "IE63aoc3bAAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_eda(df)"
      ],
      "metadata": {
        "id": "_lsYd7B6bAAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "29-tV_Nj_Deg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Variable Analysis**\n",
        "  - The target variable `y_binary` is highly imbalanced:\n",
        "    - 3,668 customers (≈89%) did not subscribe\n",
        "    - 451 customers (≈11%) did subscribe\n",
        "  - This imbalance is important because it means most customers say \"no\" — so the model needs to be especially good at identifying the rare \"yes\" cases.\n",
        "\n",
        "**Numerical Variables Analysis**\n",
        "\n",
        "**Dataset Overview**\n",
        "  - Summary statistics show:\n",
        "    - `duration` (call length) ranges from 0 to 3643 seconds — very wide spread, with a median of 181 seconds.\n",
        "    - `pdays` is 999 for most customers, meaning they were never contacted before — this value dominates the column.\n",
        "    - `campaign` (number of contacts during this campaign) has a max of 35, but most customers were contacted 1–3 times.\n",
        "    - `previous` (contacts in past campaigns) is 0 for most — very few customers were contacted before.\n",
        "\n",
        "  - Outliers and skewness:\n",
        "    - Variables like `duration`, `pdays`, and `economic_pressure_index` show strong skewness and outliers.\n",
        "    - These may need transformation or special handling in modeling.\n",
        "\n",
        "  - Distribution patterns:\n",
        "    - `duration` is positively skewed — most calls are short, but a few are very long.\n",
        "    - `euribor3m` and `nr.employed` are more normally distributed and show meaningful variation.\n",
        "\n",
        "**Business Insights (Numerical)**\n",
        "  - Longer calls are clearly linked to higher subscription rates — this suggests that meaningful conversations are more persuasive\n",
        "  - Customers who were contacted recently or had previous interactions are more likely to say yes — follow-ups matter\n",
        "  - Economic indicators like `euribor3m` (interest rate) and `emp.var.rate` (employment variation) influence decisions as customers can be more cautious in uncertain economic times\n",
        "\n",
        "**Statistical Insights (Numerical)**\n",
        "  - Top features correlated with subscription (y_binary):\n",
        "    - `duration` (0.42): Strongest predictor — longer calls = higher chance of success.\n",
        "    - `nr.employed` (0.35): Indicates that employment trends affect customer confidence.\n",
        "    - `pdays` (0.33): Lower values (i.e., recent contact) are better.\n",
        "    - `euribor3m` (0.30) and `emp.var.rate` (0.28): Reflect macroeconomic conditions.\n",
        "\n",
        "**Categorical Variables Analysis**\n",
        "\n",
        "**Dataset Overview**\n",
        "  - Job types:\n",
        "    - Most common: admin. (1,012), blue-collar (884), technician (691)\n",
        "    - Least common: student (82), housemaid (110), unemployed (111)\n",
        "  - Marital status:\n",
        "    - Majority are married (2,509), followed by single (1,153)\n",
        "  - Education:\n",
        "    - Most have university degrees (1,264) or high school education (921)\n",
        "  - Contact method:\n",
        "    - Cellular (2,652) is more common than telephone (1,467)\n",
        "  - Previous campaign outcome:\n",
        "    - Most customers were never contacted before (poutcome = nonexistent)\n",
        "\n",
        "**Business Insights (Categorical)**\n",
        "  - Higher subscription rates seen among:\n",
        "    - Students and retirees — possibly more time or interest in saving\n",
        "    - Customers with successful past outcomes — trust and familiarity help\n",
        "    - Those contacted via cellular — more personal and direct than landlines\n",
        "  - Customers with unknown values in financial fields (like `default`) tend to convert less — may signal risk or lack of transparency\n",
        "\n",
        "**Statistical Insights (Categorical)**\n",
        "  - Chi-square test results show significant relationships (p < 0.05) between subscription and:\n",
        "    - Job, marital status, education, default status, contact method, past outcome, recent contact, employment status, and age group\n",
        "  - No significant relationship with:\n",
        "    - Housing loan, personal loan, or risk group — these may not add much predictive value\n",
        "\n",
        "**Multivariate Analysis**\n",
        "  - Correlation matrix reveals:\n",
        "    - Strong positive correlation between `euribor3m` and `nr.employed` — both reflecting economic health\n",
        "    - Negative correlation between `emp.var.rate` and `pdays` — when the person isn't employed then there fewer follow-ups\n",
        "  - These relationships help identify redundant features and guide feature selection"
      ],
      "metadata": {
        "id": "wDRVDDhE-7wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-EDA\n",
        "# d = dtale.show(df, ignore_duplicate=True, port=40001)\n",
        "# d.open_browser()\n",
        "# dtale.show()"
      ],
      "metadata": {
        "id": "4OK6oPNpjtVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "Q_sy6jZobAAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seperating Inputs And Outputs"
      ],
      "metadata": {
        "id": "fBo-6bDeHaJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separating the dataframe into features (inputs) and the target (output).\n",
        "Also identifies numerical and categorical features within the inputs.\n",
        "\n",
        "Args:\n",
        "* df (pd.DataFrame): The input dataframe.\n",
        "* target_column (str): The name of the target column.\n",
        "\n",
        "Returns:\n",
        "* tuple: A tuple containing:\n",
        "  * X (pd.DataFrame): DataFrame containing the features.\n",
        "  * y (pd.Series): Series containing the target variable.\n",
        "  * numeric_feats (list): List of numerical feature names.\n",
        "  * categorical_feats (list): List of categorical feature names."
      ],
      "metadata": {
        "id": "lYVW50nLqREe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperating Inputs And Outputs\n",
        "def separate_inputs_outputs(df, target_column='y_binary'):\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(\"SEPARATING INPUTS AND OUTPUTS\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    # Define target variable\n",
        "    if target_column not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in the dataframe.\")\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Define features (excluding target and any other columns to be dropped before modeling, e.g., 'duration', 'y')\n",
        "    columns_to_drop = [target_column, 'y_binary', 'y', 'duration', 'contact_intensity']\n",
        "    # Add other columns to drop here if needed\n",
        "    x = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
        "\n",
        "    # Identify numerical and categorical features in the remaining columns (X)\n",
        "    numeric_feats = x.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_feats = x.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    print(f\"Features (x) shape: {x.shape}\")\n",
        "    print(f\"Target (y) shape: {y.shape}\")\n",
        "    print(f\"Numeric features ({len(numeric_feats)}): {numeric_feats}\")\n",
        "    print(f\"Categorical features ({len(categorical_feats)}): {categorical_feats}\")\n",
        "\n",
        "    return x, y, numeric_feats, categorical_feats"
      ],
      "metadata": {
        "id": "Cp1-4eKfoJL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, numeric_features, categorical_features = separate_inputs_outputs(df)"
      ],
      "metadata": {
        "id": "bwqfczdmrb19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Pre-Processing Pipline Set-up"
      ],
      "metadata": {
        "id": "3hN08c5rHihg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a preprocessing pipeline for numerical and categorical features.\n",
        "Args:\n",
        "* numeric_feats (list): List of names of numerical features.\n",
        "* categorical_feats (list): List of names of categorical features.\n",
        "\n",
        "Returns:\n",
        "* ColumnTransformer: A fitted ColumnTransformer object.\n"
      ],
      "metadata": {
        "id": "VPJG0qwAtScc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_preprocessing_pipeline(numeric_feats, categorical_feats):\n",
        "    # Identify features that should NOT go through Winsorizer\n",
        "    # These are likely binary features or features where outlier capping is not appropriate\n",
        "    # Also excluding pdays and previous due to low variation with IQR method\n",
        "    exclude_from_winsorizer = ['recently_contacted', 'is_employed', 'is_risk_group', 'pdays', 'previous']\n",
        "\n",
        "    # Separate numerical features into those needing winsorization and those that don't\n",
        "    numeric_feats_for_winsorizer = [feat for feat in numeric_feats if feat not in exclude_from_winsorizer]\n",
        "    numeric_feats_without_winsorizer = [feat for feat in numeric_feats if feat in exclude_from_winsorizer]\n",
        "\n",
        "\n",
        "    # Numerical pipeline for features needing winsorization: impute, winsorize outliers, and scale\n",
        "    num_pipe_winsor = Pipeline([\n",
        "        ('impute', SimpleImputer(strategy='mean')),\n",
        "        ('winsorize', Winsorizer(capping_method='iqr', tail='both', fold=1.5)),\n",
        "        ('scale', MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    # Numerical pipeline for binary-like features and others excluded from winsorizer: impute and scale (no winsorization)\n",
        "    num_pipe_no_winsor = Pipeline([\n",
        "        ('impute', SimpleImputer(strategy='mean')),\n",
        "        ('scale', MinMaxScaler()) # Scale them like other numerics\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Categorical pipeline: impute 'unknown' with mode, and one-hot encode\n",
        "    cat_pipe = Pipeline([\n",
        "         ('impute_unknown', SimpleImputer(missing_values='unknown', strategy='most_frequent')),\n",
        "        ('encode', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Combine pipelines using ColumnTransformer\n",
        "    # Ensure all identified categorical features are passed to the 'cat' transformer\n",
        "    preprocess = ColumnTransformer(transformers=[\n",
        "        ('num_winsor', num_pipe_winsor, numeric_feats_for_winsorizer),\n",
        "        ('num_no_winsor', num_pipe_no_winsor, numeric_feats_without_winsorizer),\n",
        "        ('cat', cat_pipe, categorical_feats)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    return preprocess"
      ],
      "metadata": {
        "id": "pf2RwgzBsv80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline = build_preprocessing_pipeline(numeric_features, categorical_features)\n",
        "preprocessing_pipeline"
      ],
      "metadata": {
        "id": "N2rJLwXJtlNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipline Setup Insights"
      ],
      "metadata": {
        "id": "20ucX3aCHGU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numeric features** are divided into two groups:\n",
        "  - Features like `age`, `campaign`, and `euribor3m` go through:\n",
        "    - Mean imputation for missing values\n",
        "    - Winsorization to cap extreme outliers using the IQR method\n",
        "    - Min-max scaling to bring values into a 0–1 range\n",
        "  - Binary or low-variance features like `recently_contacted`, `is_employed`, `pdays`, and `previous` are:\n",
        "    - Imputed using the mean\n",
        "    - Scaled without applying outlier treatment\n",
        "\n",
        "**Categorical features** such as `job`, `education`, and `contact` are:\n",
        "  - Imputed by replacing 'unknown' values with the most frequent category (mode)\n",
        "  - One-hot encoded to convert them into numerical format which is suitable for modeling\n",
        "\n",
        "- All transformations are combined using a ColumnTransformer, ensuring that each feature is processed appropriately and consistently.\n",
        "\n",
        "This ensures that the data is clean, scaled, and free from outliers or missing value issues."
      ],
      "metadata": {
        "id": "p8x_3odvEEss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Selection"
      ],
      "metadata": {
        "id": "JrpMVpSRHQKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**\n",
        "\n",
        "Performing feature selection using SelectKBest within a pipeline and saves the pipeline.\n",
        "\n",
        "Args:\n",
        "* x (pd.DataFrame): Feature dataframe.\n",
        "* y (pd.Series): Target series.\n",
        "* preprocess (ColumnTransformer): The preprocessing pipeline before feature selection.\n",
        "* k (int): The number of top features to select.\n",
        "* filename (str): The filename to save the pipeline.\n",
        "\n",
        "Returns:\n",
        "* np.ndarray: The feature-selected transformed feature matrix.\n",
        "* Pipeline: The fitted full pipeline including preprocessing and feature selection."
      ],
      "metadata": {
        "id": "kW0fheZvvYhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "def select_features_and_save(x, y, preprocess, k=20, filename='wind_turbine_prep_pipeline.joblib'):\n",
        "\n",
        "  print(\"\\n\" + \"*\"*50)\n",
        "  print(\"PERFORMING FEATURE SELECTION AND SAVING PIPELINE\")\n",
        "  print(\"*\"*50)\n",
        "\n",
        "  # Create the feature selection step\n",
        "  selector = SelectKBest(score_func=f_classif, k=k)\n",
        "\n",
        "  # Build the full pipeline including preprocessing and feature selection\n",
        "  full_pipe = Pipeline([\n",
        "      ('prep',  preprocess),\n",
        "      ('fs',    selector)\n",
        "  ])\n",
        "\n",
        "  # Fit-transform once to obtain clean feature matrix with selected features\n",
        "  print(f\"Fitting and transforming data with k={k} features...\")\n",
        "  x_selected = full_pipe.fit_transform(x, y)\n",
        "  print(f\"Shape of feature-selected data: {x_selected.shape}\")\n",
        "\n",
        "  # Persist the full pipeline for production reuse\n",
        "  print(f\"Saving full pipeline to '{filename}'...\")\n",
        "  joblib.dump(full_pipe, filename)\n",
        "  print(\"Pipeline saved successfully.\")\n",
        "\n",
        "  return x_selected, full_pipe"
      ],
      "metadata": {
        "id": "EJcGzFvAuzWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_selected, full_pipeline = select_features_and_save(x, y, preprocessing_pipeline, k=20, filename='bank_marketing_prep_pipeline.joblib')\n",
        "x_selected, full_pipeline"
      ],
      "metadata": {
        "id": "XpUc0xFrv8sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "FtPEAi6LHq7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Was Done**\n",
        "  - Applied a feature selection technique using **SelectKBest** to retain the top 20 most relevant features for predicting customer subscription\n",
        "  - The selection was based on statistical tests that measure how strongly each feature is related to the target variable.\n",
        "  - The full preprocessing and feature selection pipeline was saved as a reusable file: `'bank_marketing_prep_pipeline.joblib'`.\n",
        "\n",
        "**Statistical Insight**\n",
        "  - The transformed dataset now has 20 features — reducing dimensionality while keeping the most informative variables\n",
        "  - This helps:\n",
        "    - Improve model performance by removing noise\n",
        "    - Reduce overfitting\n",
        "    - Speed up training and prediction\n",
        "\n",
        "**Business Insight**\n",
        "  - By focusing only on the most predictive features, the model becomes more efficient and interpretable.\n",
        "  - Bank can better understand which customer traits or behaviors truly influence subscription decisions — leading to smarter targeting and more effective campaigns."
      ],
      "metadata": {
        "id": "5bUW3on3GKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Savinng the bank_preprocessed_data file\n",
        "bank_preprocessed_data = pd.DataFrame(x_selected)\n",
        "bank_preprocessed_data.to_csv('bank_preprocessed_data.csv', index=False)\n",
        "print(\"Processed data saved as 'bank_preprocessed_data.csv'\")"
      ],
      "metadata": {
        "id": "JYdwk3KUIfrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Split And Balancing"
      ],
      "metadata": {
        "id": "pGTh0_hu0L6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting Into Training And Test"
      ],
      "metadata": {
        "id": "d9Z2FEWQI_Gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting data into training and testing sets and applies SMOTE to the training set.\n",
        "\n",
        "Args:\n",
        "* x (np.ndarray or pd.DataFrame): The feature data.\n",
        "* y (pd.Series): The target variable.\n",
        "* test_size (float): The proportion of the dataset to include in the test split.\n",
        "* random_state (int): Controls the shuffling applied to the data before applying the split.\n",
        "\n",
        "Returns:\n",
        "* tuple: A tuple containing:\n",
        "  * x_train (np.ndarray): Training features.\n",
        "  * x_test (np.ndarray): Testing features.\n",
        "  * y_train (np.ndarray): Training target.\n",
        "  * y_test (np.ndarray): Testing target.\n"
      ],
      "metadata": {
        "id": "1ShlT4AQzqkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting The Data Into Train And Test\n",
        "def split_data_and_balance(x, y, test_size=0.20, random_state=42):\n",
        "\n",
        "  print(\"\\n\" + \"*\"*50)\n",
        "  print(\"DATA SPLIT AND BALANCING\")\n",
        "  print(\"*\"*50)\n",
        "\n",
        "  # Ensure x is a numeric NumPy array\n",
        "  x = np.asarray(x, dtype=float)\n",
        "\n",
        "  # Split data\n",
        "  print(f\"Splitting data into training ({1-test_size:.0%}) and testing ({test_size:.0%}) sets...\")\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "      x, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "  )\n",
        "  print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
        "  print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "  # Apply SMOTE to balance the training set\n",
        "  print(\"Applying SMOTE to the training data...\")\n",
        "  smote = SMOTE(random_state=random_state)\n",
        "  x_train_balanced, y_train_balanced = smote.fit_resample(x_train, y_train)\n",
        "  print(f\"X_train shape after SMOTE: {x_train_balanced.shape}, y_train shape after SMOTE: {y_train_balanced.shape}\")\n",
        "  print(f\"Training target distribution after SMOTE:\\n{pd.Series(y_train_balanced).value_counts()}\")\n",
        "\n",
        "  return x_train_balanced, x_test, y_train_balanced, y_test"
      ],
      "metadata": {
        "id": "ZbthK2D9zRMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = split_data_and_balance(x_selected, y)"
      ],
      "metadata": {
        "id": "grYQks8M0oY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "Ii3YvolKITtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Was Done**\n",
        "- The dataset was split into:\n",
        "  - 80% for training (3,295 records)\n",
        "  - 20% for testing (824 records)\n",
        "- The training data was imbalanced (fewer \"yes\" responses), so **SMOTE** (Synthetic Minority Over-sampling Technique) was applied to balance it.\n",
        "- After SMOTE:\n",
        "  - Training set size increased to 5,868 records\n",
        "  - Equal number of positive (1) and negative (0) cases: 2,934 each\n",
        "\n",
        "**Statistical Insight**\n",
        "- SMOTE created synthetic examples of the minority class (subscribers) by interpolating between existing ones\n",
        "- This helps the model learn patterns from both classes equally, rather than being biased toward the majority class\n",
        "- Balanced training data improves the model’s ability to detect rare but important outcomes (like a customer saying \"yes\").\n",
        "\n",
        "**Business Insight**\n",
        "  - In real campaigns, only a small percentage of customers subscribe — but those are the most valuable.\n",
        "  - Balancing the training data ensures the model doesn’t ignore these valuable customers.\n",
        "  - This leads to better targeting, higher conversion rates, and more efficient use of marketing resources."
      ],
      "metadata": {
        "id": "e2YON6kSIOE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building And Tuning"
      ],
      "metadata": {
        "id": "J8loqrab2CEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training And Tuning"
      ],
      "metadata": {
        "id": "HcVoPqxcLbon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Builds, tunes, and evaluates various classification models.\n",
        "\n",
        "* Args:\n",
        "  * x_train (np.ndarray): Training features.\n",
        "  * y_train (np.ndarray): Training target.\n",
        "  * x_test (np.ndarray): Testing features.\n",
        "  * y_test (np.ndarray): Testing target.\n",
        "\n",
        "* Returns:\n",
        "  * dict: A dictionary containing evaluated results for each model.\n"
      ],
      "metadata": {
        "id": "-TFsuujn5M1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building and Tuning The Model\n",
        "def build_and_tune_models(x_train, y_train, x_test, y_test):\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(\"MODEL BUILDING AND TUNING\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    models = {\n",
        "        'Logistic Regression': {\n",
        "            'model': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=40),\n",
        "            'params': {\n",
        "                'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "            }\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'model': RandomForestClassifier(class_weight='balanced', random_state=40),\n",
        "            'params': {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [None, 5, 10, 15]\n",
        "            }\n",
        "        },\n",
        "        'Gradient Boosting': {\n",
        "            'model': GradientBoostingClassifier(random_state=40),\n",
        "            'params': {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'max_depth': [3, 4, 5]\n",
        "            }\n",
        "        },\n",
        "        'K-Nearest Neighbors': {\n",
        "            'model': KNeighborsClassifier(),\n",
        "            'params': {\n",
        "                'n_neighbors': list(range(3, 20, 2))\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    best_estimators = {}\n",
        "\n",
        "    for name, model_info in models.items():\n",
        "        print(f\"\\nTraining and tuning {name}...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            model_info['model'],\n",
        "            model_info['params'],\n",
        "            cv=5,\n",
        "            scoring='f1',\n",
        "            verbose=1,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(x_train, y_train)\n",
        "\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_estimators[name] = best_model\n",
        "\n",
        "        y_pred = best_model.predict(x_test)\n",
        "        y_proba = best_model.predict_proba(x_test)[:, 1]\n",
        "\n",
        "        results[name] = {\n",
        "            'Best_model': best_model,\n",
        "            'Best_params': grid_search.best_params_,\n",
        "            'Cv_f1_score': grid_search.best_score_,\n",
        "            'Test_accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Test_precision': precision_score(y_test, y_pred),\n",
        "            'Test_recall': recall_score(y_test, y_pred),\n",
        "            'Test_f1_score': f1_score(y_test, y_pred),\n",
        "            'Test_roc_auc': roc_auc_score(y_test, y_proba),\n",
        "            'Confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "        print(f\"Test F1 score: {results[name]['Test_f1_score']:.4f}\")\n",
        "\n",
        "        filename = f'bank_marketing_{name.lower().replace(\" \",\"_\")}.pkl'\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(best_model, f)\n",
        "        print(f\"{name} model saved to '{filename}'.\")\n",
        "\n",
        "    return results, best_estimators"
      ],
      "metadata": {
        "id": "pt77BuG03w1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model building\n",
        "model_results, best_models = build_and_tune_models(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "g0urogYW6IY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "B_jmfH3hKgTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Was Done**\n",
        "  - 4 machine learning models were trained and tuned:\n",
        "    - Logistic Regression\n",
        "    - Random Forest\n",
        "    - Gradient Boosting\n",
        "    - K-Nearest Neighbors (KNN)\n",
        "  - Each model was evaluated using the F1 score, which balances precision and recall — especially important for imbalanced datasets.\n",
        "  - The best model parameters were selected based on cross-validation performance.\n",
        "  - Final models were tested on unseen data and saved for future use.\n",
        "\n",
        "**Statistical Insight**\n",
        "  - Best F1 scores on the test set:\n",
        "    - K-Nearest Neighbors: 0.4368\n",
        "    - Gradient Boosting: 0.3924\n",
        "    - Logistic Regression: 0.3889\n",
        "    - Random Forest: 0.3593\n",
        "  - KNN performed best in terms of F1 score, implying that it captured the balance between false positives and false negatives more effectively than the others\n",
        "  - Each model was tuned using a range of hyperparameters:\n",
        "    - Logistic Regression: regularization strength (C)\n",
        "    - Random Forest: number of trees and tree depth\n",
        "    - Gradient Boosting: learning rate, tree depth, and number of trees\n",
        "    - KNN: number of neighbors\n",
        "\n",
        "**Business Insight**\n",
        "  - Different models offer different strengths:\n",
        "    - KNN may be more sensitive to local patterns in customer behavior.\n",
        "    - Gradient Boosting and Logistic Regression provide more interpretable decision boundaries.\n",
        "  - By comparing multiple models, the bank can choose the one that best balances accuracy and interpretability for campaign targeting.\n",
        "  - Saving the models ensures they can be reused for real-time predictions or further analysis without retraining."
      ],
      "metadata": {
        "id": "Rb8tRyb4KXWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "rY_WgajW8LjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Evaluation"
      ],
      "metadata": {
        "id": "nGGSSKrPQhRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluates a trained classification model and prints key metrics.\n",
        "\n",
        "* Args:\n",
        "  * model: A trained scikit-learn classification model.\n",
        "  * x_test (np.ndarray or pd.DataFrame): The test features.\n",
        "  * y_test (np.ndarray or pd.Series): The true test target values.\n",
        "  * model_name (str): The name of the model for display purposes."
      ],
      "metadata": {
        "id": "ZzESt56T9SyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, x_test, y_test, model_name=\"Model\"):\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(f\"EVALUATION FOR {model_name.upper()}\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_proba = model.predict_proba(x_test)[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1 Score: {f1:.4f}\")\n",
        "    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"  Confusion Matrix:\\n\", cm)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(cm)\n",
        "    disp.plot()\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "C9mKI4zK8UFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all best models\n",
        "for name, model in best_models.items():\n",
        "    evaluate_model(model, x_test, y_test, model_name=name)"
      ],
      "metadata": {
        "id": "NN5_MaW99LX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Business And Statistical Insights"
      ],
      "metadata": {
        "id": "GJmZlZbPMUlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Metrics Used**\n",
        "\n",
        "Each model was evaluated on the test set using:\n",
        "- **Accuracy:** Overall correctness of predictions\n",
        "- **Precision:** Of all predicted positives, how many were actually positive\n",
        "- **Recall:** Of all actual positives, how many were correctly predicted\n",
        "- **F1 Score:** Harmonic mean of precision and recall — balances false positives and false negatives\n",
        "- **ROC AUC:** Measures the model’s ability to distinguish between classes\n",
        "- Confusion Matrix: Shows true vs. predicted values for both classes\n",
        "\n",
        "**Business Insight**\n",
        "\n",
        "- Logistic Regression has the highest recall (70%), meaning it catches most of the actual subscribers ie, it is useful when missing a potential customer is costly\n",
        "\n",
        "- K-Nearest Neighbors has the best F1 score (0.4368), indicating the most balanced performance between precision and recall which is ideal for general campaign targeting\n",
        "\n",
        "- Gradient Boosting offers a middle ground with good precision and recall, and high accuracy.\n",
        "\n",
        "- Random Forest has the highest accuracy but lower recall ie, it may miss many potential subscribers, which could reduce campaign effectiveness.\n",
        "\n",
        "***Since our objective is to have balanced targeting with fewer false positives we will go with the K-Nearest Neighbour Model.***\n",
        "  \n",
        "  Having the following Metrices\n",
        "  - Accuracy  0.8811\n",
        "  - Precision: 0.4524\n",
        "  - Recall: 0.4222\n",
        "  - F1 Score: 0.4368\n",
        "  - ROC AUC: 0.7167"
      ],
      "metadata": {
        "id": "NnlQK0N8MRuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability"
      ],
      "metadata": {
        "id": "miLDFB3N_YvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "# Select the best model\n",
        "best_model_name = max(model_results, key=lambda k: model_results[k]['Test_f1_score'])\n",
        "best_model = best_models[best_model_name]\n",
        "print(f\"\\nBest model selected for explainability: {best_model_name}\")"
      ],
      "metadata": {
        "id": "ivR-2DF87JOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature names matching x_test shape\n",
        "feature_names = [f\"feature_{i}\" for i in range(x_test.shape[1])]\n",
        "print(f\"Using {len(feature_names)} feature names.\")"
      ],
      "metadata": {
        "id": "B6GTtHlM7MhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert x_test to DataFrame\n",
        "sample_X_shap = pd.DataFrame(x_test, columns=feature_names)"
      ],
      "metadata": {
        "id": "aLi-7nTa7O6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define predict_proba function\n",
        "def predict_proba_best_model(arr):\n",
        "    return best_model.predict_proba(arr)[:, 1]"
      ],
      "metadata": {
        "id": "UuSIaLIF7REW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SHAP KernelExplainer\n",
        "background_data = sample_X_shap.sample(n=min(100, len(sample_X_shap)), random_state=42)\n",
        "explainer = shap.KernelExplainer(predict_proba_best_model, background_data)"
      ],
      "metadata": {
        "id": "boS38H_M7UZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute SHAP values for 500 samples\n",
        "n_samples = min(500, len(sample_X_shap))\n",
        "print(f\"\\nCalculating SHAP values for {n_samples} samples...\")\n",
        "sample_X_for_shap = sample_X_shap.sample(n=n_samples, random_state=42)\n",
        "shap_values = explainer.shap_values(sample_X_for_shap)\n",
        "print(\"SHAP value calculation complete.\")"
      ],
      "metadata": {
        "id": "G5_rzbBu7WAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzes and visualizes SHAP values for model explainability.\n",
        "\n",
        "* Args:\n",
        "  * shap_values (np.ndarray): SHAP values from the explainer.\n",
        "  * sample_X (pd.DataFrame): The subset of data used for the explainer.\n",
        "  * feature_names (np.ndarray or list): Names of the features.\n",
        "  * top_n (int): Number of top features to display in the summary plot.\n"
      ],
      "metadata": {
        "id": "E8DOFBHiARBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze and visualize SHAP values\n",
        "def analyze_shap_values(shap_values, sample_x, feature_names, explainer, top_n=10):\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(\"ANALYZING SHAP VALUES\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    if not isinstance(sample_x, pd.DataFrame):\n",
        "        sample_x = pd.DataFrame(sample_x, columns=feature_names)\n",
        "\n",
        "    # SHAP summary plot\n",
        "    print(f\"Generating SHAP summary plot (top {top_n} features)...\")\n",
        "    shap.summary_plot(shap_values, sample_x, show=False, max_display=top_n)\n",
        "    plt.title('SHAP Summary Plot')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance\n",
        "    if isinstance(shap_values, list):\n",
        "        mean_abs = np.abs(shap_values[1]).mean(axis=0)\n",
        "    else:\n",
        "        mean_abs = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "    importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Mean_Abs_SHAP': mean_abs\n",
        "    }).sort_values(by='Mean_Abs_SHAP', ascending=False)\n",
        "    print(\"\\nTop features by mean absolute SHAP value:\")\n",
        "    print(importance.head(top_n))\n",
        "\n",
        "    # SHAP force plots for the first 5 samples\n",
        "    print(\"\\nGenerating SHAP force plots for first 5 samples...\")\n",
        "    for i in range(min(5, len(sample_x))):\n",
        "        shap.force_plot(\n",
        "            explainer.expected_value,\n",
        "            shap_values[i],\n",
        "            sample_x.iloc[i],\n",
        "            feature_names=feature_names,\n",
        "            matplotlib=True,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Force Plot for Sample {i}')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "sRKJD9ar6Gcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run SHAP analysis\n",
        "analyze_shap_values(shap_values, sample_X_for_shap, feature_names, explainer)"
      ],
      "metadata": {
        "id": "XKU4ET6E7FTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP Explainability Insights for K-Nearest Neighbors**\n",
        "\n",
        "**Top 10 Most Influential Features (by SHAP value)**\n",
        "\n",
        "| Rank | Feature       | SHAP Impact | Interpretation |\n",
        "|------|---------------|-------------|----------------|\n",
        "| 1    | feature_3     | 0.0822      | Highest influence on predictions |\n",
        "| 2    | feature_2     | 0.0461      | Strong secondary driver |\n",
        "| 3    | feature_14    | 0.0450      | Close third in importance |\n",
        "| 4    | feature_0     | 0.0410      | Meaningful contributor |\n",
        "| 5    | feature_4     | 0.0388      | Moderate influence |\n",
        "| 6    | feature_1     | 0.0325      | Moderate influence |\n",
        "| 7–10 | features 11, 10, 5, 17 | ~0.01–0.014 | Lower but still relevant |\n",
        "\n",
        "**SHAP Summary Plot**\n",
        "- **Horizontal spread**: The wider the spread of SHAP values for a feature, the more impact it has on predictions\n",
        "- **Color gradient**:\n",
        "  - Red dots = high feature values\n",
        "  - Blue dots = low feature values\n",
        "  - If red dots are mostly on the right (positive SHAP values), high values of that feature increase the likelihood of subscription\n",
        "  - If red dots are on the left, high values reduce the likelihood\n",
        "\n",
        "**Business Insight**\n",
        "- SHAP shows which features drive predictions\n",
        "- This helps marketers:\n",
        "  - Focus on the traits that matter most\n",
        "  - Personalize outreach strategies based on what influences different customer segments"
      ],
      "metadata": {
        "id": "RjaNP3x7vn9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-To-End Pipline Complete"
      ],
      "metadata": {
        "id": "foleoPL0VYtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files generated:\n",
        "\n",
        "* • bank_marketing_prep_pipeline.joblib    (pre-processing + FS)\n",
        "* • bank_marketing_k-nearest_neighbors.pkl (optimised classifier)\n",
        "* • SHAP summary plot                      (feature importance)\n"
      ],
      "metadata": {
        "id": "snjZqcEfCW1r"
      }
    }
  ]
}